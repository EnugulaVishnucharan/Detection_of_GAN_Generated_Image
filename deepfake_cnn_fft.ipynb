{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9454e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torchvision\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import torch.utils.data as dt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Add the deepfakedetection directory to the path if needed\n",
    "sys.path.append('/home/vishnu/AAT/Self/deepfakedetection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda85d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e06af332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the paths to point to the deepfakedetection directory\n",
    "DIR_DATA_INFO = '/home/vishnu/AAT/Self/deepfakedetection/140k-real-and-fake-faces/'\n",
    "\n",
    "DIR_DATA = '/home/vishnu/AAT/Self/deepfakedetection/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/'\n",
    "DIR_DATA_TPDNE = '/home/vishnu/AAT/Self/deepfakedetection/tpdne-60k-128x128/'\n",
    "DIR_DATA_CELEBA = '/home/vishnu/AAT/Self/deepfakedetection/celeba-dataset/img_align_celeba/img_align_celeba/'\n",
    "\n",
    "URL_DATA = 'https://www.kaggle.com/datasets/xhlulu/140k-real-and-fake-faces'\n",
    "URL_DATA_TPDNE = 'https://www.kaggle.com/datasets/potatohd404/tpdne-60k-128x128'\n",
    "URL_DATA_CELEBA = 'https://www.kaggle.com/jessicali9530/celeba-dataset'\n",
    "\n",
    "LABEL_FAKE = 0\n",
    "LABEL_REAL = 1\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "TEST_SIZE = 1000\n",
    "\n",
    "DATA_TYPES = ['train', 'test', 'valid']\n",
    "\n",
    "def get_data_info_file(data_type: str, data_info_dir: List[str]=DIR_DATA_INFO) -> str:\n",
    "    assert data_type in DATA_TYPES\n",
    "    return os.path.join(data_info_dir, f'{data_type}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0617bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N = x.shape[0]\n",
    "        return x.view(N, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d13f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model() -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "           nn.Conv2d(3, 32, kernel_size=7, padding=3),\n",
    "           nn.ReLU(),\n",
    "           nn.Dropout2d(p=0.2),\n",
    "           nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "           nn.ReLU(),\n",
    "           nn.Dropout2d(p=0.2),\n",
    "           nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "           nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "           nn.ReLU(),\n",
    "           nn.Dropout2d(p=0.2),\n",
    "           nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "           nn.ReLU(),\n",
    "           nn.Dropout2d(p=0.2),\n",
    "           nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "           Flatten(),\n",
    "           nn.Linear(128 * (IMAGE_SIZE // 16) ** 2, 1),\n",
    "           nn.Sigmoid(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99d7ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_info(data_type, source='local'):\n",
    "  if source == 'local':\n",
    "    data_info = pd.read_csv(get_data_info_file(data_type))\n",
    "    data_info['image'] = data_info['path'].apply(lambda path: os.path.join(DIR_DATA, path))\n",
    "    data_info['label'] = data_info['label'].astype(int)\n",
    "    if data_type == 'test':\n",
    "      data_info = data_info.sample(TEST_SIZE)\n",
    "    return data_info[['image', 'label']]\n",
    "  \n",
    "  if source == 'tpdne':\n",
    "    data_info = pd.DataFrame(columns=['image', 'label'])\n",
    "    data_info['image'] = pd.Series([f\"{DIR_DATA_TPDNE}/{f_img}\" for f_img in os.listdir(DIR_DATA_TPDNE)[:TEST_SIZE]])\n",
    "    data_info['label'] = LABEL_FAKE\n",
    "    return data_info\n",
    "  \n",
    "  if source == 'celeba':\n",
    "    data_info = pd.DataFrame(columns=['image', 'label'])\n",
    "    data_info['image'] = pd.Series([f\"{DIR_DATA_CELEBA}/{f_img}\" for f_img in os.listdir(DIR_DATA_CELEBA)[:TEST_SIZE]])\n",
    "    data_info['label'] = LABEL_REAL\n",
    "    return data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "178119f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceImageDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_type: str,\n",
    "            mean: torch.Tensor=0,\n",
    "            std: torch.Tensor=1,\n",
    "            do_augment: bool=False,\n",
    "            do_fft: bool=False,\n",
    "            source: str='local'):\n",
    "        self.data_type = data_type\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.do_augment = do_augment\n",
    "        self.do_fft = do_fft\n",
    "        self.source = source\n",
    "\n",
    "        image_info = load_data_info(data_type, source)\n",
    "        self.image_files = image_info['image'].values\n",
    "        self.image_labels = image_info['label'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_files[idx])\n",
    "\n",
    "        augment = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            torchvision.transforms.RandomRotation(180),\n",
    "            torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            torchvision.transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n",
    "        ])\n",
    "        fft = lambda x: torch.fft.fft2(x).abs()\n",
    "        tensorize = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "        ])\n",
    "        normalize = torchvision.transforms.Normalize(mean=self.mean, std=self.std)\n",
    "\n",
    "        if self.do_augment:\n",
    "            image = augment(image)\n",
    "\n",
    "        image = tensorize(image)\n",
    "                \n",
    "        if self.do_fft:\n",
    "            image = fft(image)\n",
    "\n",
    "        image = normalize(image)\n",
    "\n",
    "        image = image.to(device)\n",
    "        label = torch.tensor(self.image_labels[idx], dtype=torch.float).expand(1).to(device)\n",
    "            \n",
    "        return (image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d7c181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_mean_and_std(dataset):\n",
    "#     loader = dt.DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "#     mean = torch.zeros(3)\n",
    "#     std = torch.zeros(3)\n",
    "#     num_samples = 0.0\n",
    "\n",
    "#     for (image, _) in tqdm(loader):\n",
    "#         batch_samples = image.size(0)\n",
    "#         color_channels = image.size(1)\n",
    "#         image = image.view(batch_samples, color_channels, -1)\n",
    "#         mean += image.mean(2).sum(0)\n",
    "#         std += image.std(2).sum(0)\n",
    "#         num_samples += batch_samples\n",
    "\n",
    "#     mean /= num_samples\n",
    "#     std /= num_samples\n",
    "\n",
    "#     mean = mean.to(device)\n",
    "#     std = std.to(device)\n",
    "\n",
    "#     return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc5be292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Getting mean and std...\")\n",
    "# mean, std = compute_mean_and_std(FaceImageDataset('train'))\n",
    "# mean_fft, std_fft = compute_mean_and_std(FaceImageDataset('train', do_fft=True))\n",
    "# mean_aug, std_aug = compute_mean_and_std(FaceImageDataset('train', do_augment=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77de92f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.tensor([0.5209, 0.4259, 0.3807])\n",
    "std = torch.tensor([0.2399, 0.2145, 0.2120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9d3c16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name, f_model, do_fft, do_aug, mean, std = 'cnn-aug', cnn_model, True, False, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f4733a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: cnn-aug, FFT: True, Augmentation: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model: {model_name}, FFT: {do_fft}, Augmentation: {do_aug}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6c219e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = f_model()\n",
    "model = model.to(device)\n",
    "train_data = FaceImageDataset('train', mean=mean, std=std, do_fft=do_fft, do_augment=do_aug)\n",
    "val_data = FaceImageDataset('valid', mean=mean, std=std, do_fft=do_fft, do_augment=do_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58b694ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "  (1): ReLU()\n",
       "  (2): Dropout2d(p=0.2, inplace=False)\n",
       "  (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): ReLU()\n",
       "  (5): Dropout2d(p=0.2, inplace=False)\n",
       "  (6): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU()\n",
       "  (9): Dropout2d(p=0.2, inplace=False)\n",
       "  (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU()\n",
       "  (12): Dropout2d(p=0.2, inplace=False)\n",
       "  (13): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  (14): Flatten()\n",
       "  (15): Linear(in_features=2048, out_features=1, bias=True)\n",
       "  (16): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0405c268",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_EPOCHS = 12\n",
    "BATCH_SIZE = 64\n",
    "SAVED_MODELS_DIR = \"saved_models\"\n",
    "LOGS_DIR = \"logs\"\n",
    "\n",
    "def get_timestamp() -> str:\n",
    "   return datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "def get_saved_model_path(model_name: str, timestamp: str) -> str:\n",
    "    return f\"{SAVED_MODELS_DIR}/{model_name}/{timestamp}.pt\"\n",
    "\n",
    "def get_log_dir(model_name: str, timestamp: str) -> str:\n",
    "    return f\"{LOGS_DIR}/{model_name}/{timestamp}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c211cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model: nn.Module, dataset: dt.Dataset):\n",
    "  model = model.to(device)  # Move model to device\n",
    "  model.eval()\n",
    "  dataloader = dt.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "  with torch.no_grad():\n",
    "    acc = 0\n",
    "    num_samples = 0\n",
    "    for (x, y) in tqdm(dataloader):\n",
    "      x, y = x.to(device), y.to(device)  # Move data to the same device as model\n",
    "      scores = model(x)\n",
    "      y_pred = scores > 0.5\n",
    "      acc += torch.count_nonzero(y_pred == y)\n",
    "      num_samples += len(y)\n",
    "  model.train()\n",
    "  return acc / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49147c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name: str, model: nn.Module, train_data: dt.Dataset, val_data: dt.Dataset):\n",
    "  timestamp = get_timestamp()\n",
    "  model_save_path = get_saved_model_path(model_name, timestamp)\n",
    "  log_dir = get_log_dir(model_name, timestamp)\n",
    "  os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "  \n",
    "  model = model.to(device)\n",
    "  \n",
    "  writer = SummaryWriter(log_dir=log_dir)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "  criterion = nn.BCELoss().to(device)\n",
    "  dataloader = dt.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "  lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.2)\n",
    "\n",
    "  last_val_acc = 0\n",
    "  val_acc_decreasing_counter = 0\n",
    "  global_step = 0\n",
    "  model.train()\n",
    "  for epoch in range(TRAIN_EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{TRAIN_EPOCHS}\")\n",
    "    torch.save(model.state_dict(), model_save_path)   \n",
    "\n",
    "    for data in tqdm(dataloader):\n",
    "      optimizer.zero_grad()\n",
    "      x, y = data\n",
    "      x, y = x.to(device), y.to(device)  # Move data to the same device as model\n",
    "      scores = model(x)\n",
    "      y_pred = scores > 0.5\n",
    "      acc = torch.count_nonzero(y_pred == y) / len(y_pred)\n",
    "      loss = criterion(scores, y)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      writer.add_scalar(\"loss\", loss.item(), global_step=global_step)\n",
    "      writer.add_scalar(\"acc/train\", acc, global_step=global_step)\n",
    "\n",
    "      global_step += 1\n",
    "\n",
    "    val_acc = test_model(model, val_data)\n",
    "    if val_acc < last_val_acc:\n",
    "      val_acc_decreasing_counter += 1\n",
    "    else:\n",
    "      val_acc_decreasing_counter = 0\n",
    "\n",
    "    writer.add_scalar(\"acc/val\", val_acc, global_step=global_step)\n",
    "    lr_scheduler.step()\n",
    "    writer.flush()\n",
    "\n",
    "    if val_acc_decreasing_counter >= 3:\n",
    "      print(\"Stopping early due to decreasing validation accuracy\")\n",
    "      break\n",
    "\n",
    "  torch.save(model.state_dict(), model_save_path)   \n",
    "  writer.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d0c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training {model_name}...\")\n",
    "train_model(model_name, model, train_data, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663da994",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb4e4da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODEL_DIR = 'saved_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48001422",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SPECS = [\n",
    "  # ('normal', 'train', 0, 1, False, False),\n",
    "  # ('aug', 'train', 0, 1, True, False),\n",
    "  ('fft', 'train', 0, 1, False, True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebcc42b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECS = [\n",
    "  # ('cnn', cnn_model, False, False, mean, std),\n",
    "  # ('cnn-aug', cnn_model, False, True, mean, std),\n",
    "  ('cnn-fft', cnn_model, True, False, mean, std),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f65411b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_recent_model(model):\n",
    "  model_dir = f'{SAVED_MODEL_DIR}/{model}'\n",
    "  if not os.path.exists(model_dir):\n",
    "    return None\n",
    "  files = os.listdir(model_dir)\n",
    "  if not files:\n",
    "    return None\n",
    "  return f'{model_dir}/{sorted(files)[-1]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93ece78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_MODELS = [\n",
    "  model for model in [\n",
    "    # ('cnn', get_most_recent_model('cnn')),\n",
    "    # ('cnn-aug', get_most_recent_model('cnn-aug')),\n",
    "    ('cnn-fft', get_most_recent_model('cnn-fft')),\n",
    "  ] if model[1] is not None\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0763cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor):\n",
    "  image = tensor.cpu().numpy().transpose((1, 2, 0))\n",
    "  image = (image * 255).astype('uint8')\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17300bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(img, path):\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "  if not os.path.exists(os.path.dirname(path)):\n",
    "    os.makedirs(os.path.dirname(path))\n",
    "  cv2.imwrite(path, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "181bf6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash_tensor(tensor):\n",
    "  return torch.clip(tensor / 100, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464312e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_images():\n",
    "  fake_images = []\n",
    "  real_images = []\n",
    "  dataset = FaceImageDataset('train')\n",
    "  while len(fake_images) < 10 or len(real_images) < 10:\n",
    "    i = np.random.randint(len(dataset))\n",
    "    image, label = dataset[i]\n",
    "    if label == 0:\n",
    "      if len(fake_images) < 10:\n",
    "        fake_images.append(i)\n",
    "    else:\n",
    "      if len(real_images) < 10:\n",
    "        real_images.append(i)\n",
    "  \n",
    "  for spec in DATASET_SPECS:\n",
    "    name, args = spec[0], spec[1:]\n",
    "    dataset = FaceImageDataset(*args)\n",
    "    for idx, i in enumerate(fake_images):\n",
    "      image, _ = dataset[i]\n",
    "      if name == 'fft':\n",
    "        image = squash_tensor(image)\n",
    "      plot_image(tensor_to_image(image), f'plots/sample-images/{name}/fake/{idx}.png')\n",
    "\n",
    "    for i in real_images:\n",
    "      image, _ = dataset[i]\n",
    "      if name == 'fft':\n",
    "        image = squash_tensor(image)\n",
    "      plot_image(tensor_to_image(image), f'plots/sample-images/{name}/real/{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cbb46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models():\n",
    "  df = pd.DataFrame(columns=['model', 'accuracy'])\n",
    "  for source in ['local', 'tpdne', 'celeba']:\n",
    "    for spec in SPECS:\n",
    "      model_name, f_model, do_fft, _, mean, std = spec\n",
    "      print(f\"Evaluating model {model_name} on dataset {source}...\")\n",
    "      data = FaceImageDataset('test', source=source, do_fft=do_fft, do_augment=False, mean=mean, std=std)\n",
    "      model = f_model()\n",
    "      f_saved_model = os.listdir(f\"../deepfakedetection/{SAVED_MODELS_DIR}/{model_name}\")[0]\n",
    "\n",
    "      state_dict = torch.load(f\"../deepfakedetection/{SAVED_MODELS_DIR}/{model_name}/{f_saved_model}\", map_location=\"cpu\")\n",
    "\n",
    "      # create new OrderedDict without \"module.\"\n",
    "      new_state_dict = OrderedDict()\n",
    "      for k, v in state_dict.items():\n",
    "          name = k.replace(\"module.\", \"\")  # remove `module.`\n",
    "          new_state_dict[name] = v\n",
    "\n",
    "      model.load_state_dict(new_state_dict)\n",
    "      acc = test_model(model, data)\n",
    "      print(f\"Accuracy: {acc.item()}\")\n",
    "      df = df._append({'model': model_name, 'accuracy': acc.item(), 'source': source}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c8ce250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model cnn-fft on dataset local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:03<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6589999794960022\n",
      "Evaluating model cnn-fft on dataset tpdne...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:02<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.578000009059906\n",
      "Evaluating model cnn-fft on dataset celeba...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:02<00:00,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9240000247955322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cnn_filters():\n",
    "  model = cnn_model()\n",
    "  for name, path in CNN_MODELS:\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    tensor = model.module[0].weight.data.clone().cpu()  # Move to CPU\n",
    "    nrow = 8\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=1)\n",
    "    plt.figure( figsize=(nrow,rows) )\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{name} Shallow CNN 7x7 filters\")\n",
    "    if not os.path.exists('plots/cnn-filters'):\n",
    "      os.makedirs('plots/cnn-filters')\n",
    "    plt.savefig(f'plots/cnn-filters/{name}.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
